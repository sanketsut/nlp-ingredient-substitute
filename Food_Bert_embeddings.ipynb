{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Food_Bert_embeddings.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM861N7QzKZTopjWuEhps4C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dI18FDciwAs7"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd '/content/gdrive/MyDrive/ingredient substitution'"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"wjcICx6awCXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","import torch\n","import json\n","import re\n","import nltk\n","from nltk import sent_tokenize,word_tokenize\n","nltk.download('punkt')\n","from operator import add\n","import pickle"],"metadata":{"id":"-W1iFaIhwJrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load custom ingredients lists and custom vocab\n","with open('./Food_BERT_model/used_ingredients.json', 'r') as f:\n","    used_ingredients = json.load(f)\n","tokenizer = BertTokenizer(vocab_file='./Food_BERT_model/bert-base-cased-vocab.txt', do_lower_case=False, max_len=256, never_split=used_ingredients)"],"metadata":{"id":"u-_D6eNVwiMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ingredien lit without the underscore\n","used_ingredients_without_underscore = []\n","for ing in used_ingredients:\n","  result = re.sub('_', ' ', ing)  \n","  used_ingredients_without_underscore.append(result)"],"metadata":{"id":"FAVHEkxtyPjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#read file\n","# empty list to read list from a file\n","final_datalist = []\n","\n","# open file and read the content in a list\n","with open(r'./data/1100kinstruction.txt', 'r') as fp:\n","    for line in fp:\n","        x = line[:-1]\n","        final_datalist.append(x)"],"metadata":{"id":"Zisnl8WEyVwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# due to lack of computational resources we demonstrate our approach with just using first 5K recipes.\n","data = final_datalist[:5000]"],"metadata":{"id":"C8WAc1gzyhkY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Function to create windows of length n in the text. Us this function to check if ingredient in in used_ingredient list is exactly present in text or not.\n","# We cannot use \"in\" to check because it will also return true if word is present as subword of another word but instead we want to check for presence of exact word \n","def create_window(lst,n):\n","  batch_list = []\n","  for i in range(len(lst)-n+1):\n","    batch = lst[i:i+n]\n","    batch_list.append(batch)\n","  return batch_list"],"metadata":{"id":"7wAU0RoIy0TB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def last_layer(data):\n","  model = BertModel.from_pretrained(pretrained_model_name_or_path='./Food_BERT_model/checkpoint')\n","  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","  model.to(device)\n","  i=1\n","  embedding_dt = {}\n","  for data in test:\n","    if(i % 250 == 0):\n","      print(i)\n","    i = i+1\n","    sent_list = sent_tokenize(data)\n","    for sent in sent_list:\n","      sent = re.sub(u'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',sent)\n","      sent = sent.lower()\n","      tokenized_sent = word_tokenize(sent)\n","      for ing in used_ingredients_without_underscore:\n","        current = False\n","        tokenized_ing = word_tokenize(ing)\n","        l = len(tokenized_ing)\n","        #create a window of length l in sentence\n","        sentence_window = create_window(tokenized_sent,l)\n","        #if any of the window is same as tokenized ing than do the loop\n","        if(tokenized_ing in sentence_window):\n","          word = re.sub(' ', '_',ing)\n","          word_id = tokenizer.convert_tokens_to_ids(word)\n","          new_sent = re.sub(ing,word,sent)\n","          input_ids = torch.tensor(tokenizer.encode(new_sent, add_special_tokens=True)).unsqueeze(0)  # Unsqueeze because batch size 1\n","          input_ids = input_ids.to(device)\n","          outputs = model(input_ids)\n","          all_token_emb = torch.squeeze(outputs[0], 0)\n","          if(word_id in input_ids[0].tolist()):\n","            pos = input_ids[0].tolist().index(word_id)\n","            if word in embedding_dt:\n","              embedding_dt[word] = [x / 2 for x in map(add, embedding_dt[word],all_token_emb[pos].tolist())]\n","            else:\n","              embedding_dt[word] = all_token_emb[pos].tolist()\n","  return embedding_dt"],"metadata":{"id":"zk-7abekyAS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mean_layers(data):\n","  model = BertModel.from_pretrained(pretrained_model_name_or_path='foodbert/data/mlm_output/checkpoint-final',output_hidden_states = True)\n","  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","  model.to(device)\n","  embedding_dt_mean = {}\n","  k=1\n","  for data in test:\n","    if(k % 250 == 0):\n","      print(k)\n","    k = k+1\n","    sent_list = sent_tokenize(data)\n","    for sent in sent_list:\n","      sent = re.sub(u'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',sent)\n","      sent = sent.lower()\n","      tokenized_sent = word_tokenize(sent)\n","      for ing in used_ingredients_without_underscore:\n","        current = False\n","        tokenized_ing = word_tokenize(ing)\n","        l = len(tokenized_ing)\n","        #create a window of length l in sentence\n","        sentence_window = create_window(tokenized_sent,l)\n","        #if any of the window is same as tokenized ing than do the loop\n","        if(tokenized_ing in sentence_window):\n","          word = re.sub(' ', '_',ing)\n","          word_id = tokenizer.convert_tokens_to_ids(word)\n","          new_sent = re.sub(ing,word,sent)\n","          input_ids = torch.tensor(tokenizer.encode(new_sent, add_special_tokens=True)).unsqueeze(0)  # Unsqueeze because batch size 1\n","          input_ids = input_ids.to(device)\n","          outputs = model(input_ids)\n","          outputs = outputs[2]\n","          emb_list = []\n","          for i in range(len(outputs)):\n","            emb_list.append(outputs[i][0])\n","          start = emb_list[0]\n","          for i in range(1,len(emb_list)):\n","            start = torch.add(start,emb_list[i])\n","          all_token_emb = torch.div(start,len(emb_list))\n","          if(word_id in input_ids[0].tolist()):\n","            pos = input_ids[0].tolist().index(word_id)\n","            if word in embedding_dt_mean:\n","              embedding_dt_mean[word] = [x / 2 for x in map(add, embedding_dt_mean[word],all_token_emb[pos].tolist())]\n","            else:\n","              embedding_dt_mean[word] = all_token_emb[pos].tolist()\n","  return embedding_dt_mean"],"metadata":{"id":"jVMiXkBz0T3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def weighted_layers(data):\n","  model = BertModel.from_pretrained(pretrained_model_name_or_path='foodbert/data/mlm_output/checkpoint-final',output_hidden_states = True)\n","  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","  model.to(device)\n","  embedding_dt_weighted = {}\n","  k=1\n","  weights = [0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.1,0.2,0.6]\n","  for data in test:\n","    if(k % 250 == 0):\n","      print(k)\n","    k = k+1\n","    sent_list = sent_tokenize(data)\n","    for sent in sent_list:\n","      #print(sent)\n","      sent = re.sub(u'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',sent)\n","      sent = sent.lower()\n","      tokenized_sent = word_tokenize(sent)\n","      for ing in used_ingredients_without_underscore:\n","        current = False\n","        tokenized_ing = word_tokenize(ing)\n","        l = len(tokenized_ing)\n","        #create a window of length l in sentence\n","        sentence_window = create_window(tokenized_sent,l)\n","        #if any of the window is same as tokenized ing than do the loop\n","        if(tokenized_ing in sentence_window):\n","          word = re.sub(' ', '_',ing)\n","          word_id = tokenizer.convert_tokens_to_ids(word)\n","          new_sent = re.sub(ing,word,sent)\n","          input_ids = torch.tensor(tokenizer.encode(new_sent, add_special_tokens=True)).unsqueeze(0)  # Unsqueeze because batch size 1\n","          input_ids = input_ids.to(device)\n","          outputs = model(input_ids)\n","          outputs = outputs[2]\n","          emb_list = []\n","          for i in range(len(outputs)):\n","            emb_list.append(outputs[i][0])\n","          start = torch.mul(emb_list[0],weights[0])\n","          for i in range(1,len(emb_list)):\n","            start = torch.add(start,torch.mul(emb_list[i],weights[i]))\n","          all_token_emb = start\n","          if(word_id in input_ids[0].tolist()):\n","            pos = input_ids[0].tolist().index(word_id)\n","            if word in embedding_dt_weighted:\n","              embedding_dt_weighted[word] = [x / 2 for x in map(add, embedding_dt_weighted[word],all_token_emb[pos].tolist())]\n","            else:\n","              embedding_dt_weighted[word] = all_token_emb[pos].tolist()\n","  return embedding_dt_weighted"],"metadata":{"id":"2dFHJxuyOBP7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_dt = last_layer(data)"],"metadata":{"id":"tvRCybNAPbkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save file\n","with open('/content/gdrive/MyDrive/ingredient substitution/embedding_last_layer_5K.pkl', 'wb') as f:\n","    pickle.dump(embedding_dt, f)"],"metadata":{"id":"j4JvqkepO6XX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_dt_mean = mean_layers(data)"],"metadata":{"id":"UiuiihrHPuCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save file\n","with open('/content/gdrive/MyDrive/ingredient substitution/embedding_mean_layer_5K.pkl', 'wb') as f:\n","    pickle.dump(embedding_dt_mean, f)"],"metadata":{"id":"BHHQgbpIPYin"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_dt_weighted = weighted_layers(data)"],"metadata":{"id":"-oPcjg7wPe4g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save file\n","with open('/content/gdrive/MyDrive/ingredient substitution/embedding_weighted_layer_5K.pkl', 'wb') as f:\n","    pickle.dump(embedding_dt_weighted, f)"],"metadata":{"id":"pc5sN8iqPY0M"},"execution_count":null,"outputs":[]}]}